{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, using CPU instead.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Path of Dataset\n",
    "data_path = './train_valid_set/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## definition of dataset\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data,labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "        self.data = np.array(self.data).astype(np.float32)\n",
    "\n",
    "        # Convert the data to a PyTorch tensor\n",
    "        self.data = torch.tensor(self.data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.labels[index]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## definition of network. Refer to implement of TCN(https://github.com/LOCUSLAB/tcn)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.parametrizations import weight_norm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Chomp1d(nn.Module):\n",
    "    def __init__(self, chomp_size):\n",
    "        super(Chomp1d, self).__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:, :, :-self.chomp_size].contiguous()\n",
    "\n",
    "\n",
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation))\n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation))\n",
    "        self.chomp2 = Chomp1d(padding)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n",
    "                                 self.conv2, self.chomp2, self.relu2, self.dropout2)\n",
    "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "        self.relu = nn.ReLU()\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.conv1.weight.data.normal_(0, 0.01)\n",
    "        self.conv2.weight.data.normal_(0, 0.01)\n",
    "        if self.downsample is not None:\n",
    "            self.downsample.weight.data.normal_(0, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.relu(out + res)\n",
    "\n",
    "\n",
    "class TemporalConvNet(nn.Module):\n",
    "    def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2):\n",
    "        super(TemporalConvNet, self).__init__()\n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n",
    "                                     padding=(kernel_size-1) * dilation_size, dropout=dropout)]\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "    \n",
    "class TCN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_channels, kernel_size, dropout):\n",
    "        super(TCN, self).__init__()\n",
    "        self.tcn = TemporalConvNet(input_size, num_channels, kernel_size=kernel_size, dropout=dropout)\n",
    "        self.linear = nn.Linear(num_channels[-1], output_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Inputs have to have dimension (N, C_in, L_in)\"\"\"\n",
    "        y1 = self.tcn(inputs.transpose(1, 2))  # input should have dimension (N, C, L)\n",
    "        o = self.linear(y1[:, :, -1])\n",
    "        return F.log_softmax(o, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load data and create a model\n",
    "def load_data(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip().split(' ')\n",
    "            line = [float(x) for x in line]\n",
    "            data.append(line)\n",
    "    return np.array(data)\n",
    "\n",
    "def pad_sample(sample, min_len):\n",
    "  padded_sample = sample[0:min_len]\n",
    "  return padded_sample\n",
    "\n",
    "def normalize_sample(sample):\n",
    "    for dim in range(3):\n",
    "        channel = sample[:,dim]\n",
    "        min_val = np.min(channel)\n",
    "        max_val = np.max(channel)\n",
    "        normalized_channel = (channel - min_val) / (max_val - min_val)\n",
    "        sample[:, dim] = normalized_channel\n",
    "    return sample\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "with open(data_path+'index.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        line = line.strip().split(' ')\n",
    "        file_path = data_path+line[0]\n",
    "        label = int(line[1])\n",
    "        sample_data = load_data(file_path)\n",
    "        data.append(sample_data)\n",
    "        labels.append(label)\n",
    "\n",
    "# operation of train set\n",
    "min_len = min([len(sample) for sample in data])\n",
    "data = [(pad_sample(sample, min_len)) for sample in data]\n",
    "min_val = np.min(labels)\n",
    "max_val = np.max(labels)\n",
    "labels =labels -min_val\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(data, labels, test_size = 0.2 , random_state = 0)\n",
    "\n",
    "## count the num of diff labels in dataset\n",
    "label_counts = {}\n",
    "for lable in labels:\n",
    "    if lable in label_counts:\n",
    "        label_counts[lable] += 1\n",
    "    else:\n",
    "        label_counts[lable] = 1\n",
    "print(label_counts)\n",
    "\n",
    "print(len(X_train))\n",
    "print(len(X_valid))\n",
    "\n",
    "### create dataset\n",
    "train_dataset = MyDataset(X_train,y_train)\n",
    "valid_dataset = MyDataset(X_valid,y_valid)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam, AdamW\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score\n",
    "from PiH_utils import checkpoint\n",
    "\n",
    "np.random.seed(4)\n",
    "torch.manual_seed(6)\n",
    "random.seed(5)\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 5e-5\n",
    "num_epochs = 800\n",
    "num_classes = 7\n",
    "batch_size = 8\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Model instantiation\n",
    "model = TCN(3, num_classes, [16, 32, 64], kernel_size=4, dropout=0.01).to(device)\n",
    "loss_function = nn.CrossEntropyLoss()  # Assuming a classification problem\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train the model\n",
    "ckp_epoch =  0  \n",
    "best_f1 = 0  \n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  \n",
    "    total_loss = 0\n",
    "\n",
    "    for sequences, labels in train_loader:\n",
    "        \n",
    "        labels = labels.type(torch.LongTensor)\n",
    "\n",
    "        sequences, labels = sequences.to(device), labels.to(device)\n",
    "        sequences = sequences.float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(sequences)\n",
    "\n",
    "        loss = loss_function(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    print(f'Epoch [{epoch+ckp_epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader):.4f}')\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for sequences, labels in val_loader:\n",
    "            sequences, labels = sequences.to(device), labels.to(device)\n",
    "            sequences = sequences.float()\n",
    "\n",
    "            outputs = model(sequences)\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "    if f1 >= best_f1:\n",
    "      best_f1 = f1\n",
    "      print(f\"Best F1: {f1:.4f}\")\n",
    "      checkpoint(best_f1, epoch+ckp_epoch,model,\"./model/\")\n",
    "      best_model = model\n",
    "    print(f'Epoch [{epoch+ckp_epoch+1}/{num_epochs}], Test F1: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Testing\n",
    "real_path = './testing_set/'\n",
    "real_data = []\n",
    "real_labels = []\n",
    "with open(real_path+'index.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        line = line.strip().split(' ')\n",
    "        file_path = real_path+line[0]\n",
    "        # print(line[1])\n",
    "        label = int(line[1])\n",
    "        sample_data = load_data(file_path)\n",
    "        real_data.append(sample_data)\n",
    "        real_labels.append(label)\n",
    "\n",
    "min_len = min([len(sample) for sample in real_data])\n",
    "real_data = [(pad_sample(sample, min_len)) for sample in real_data]\n",
    "real_labels =real_labels -min_val\n",
    "\n",
    "real_dataset = MyDataset(real_data,real_labels)\n",
    "real_loader = DataLoader(real_dataset, batch_size=1, shuffle=False)\n",
    "print(len(real_labels))\n",
    "\n",
    "model = TCN(3, num_classes, [16, 32, 64], kernel_size=4, dropout=0.01).to(device)\n",
    "ckpt = torch.load('./model/model1230.pth.tar')\n",
    "model.load_state_dict(ckpt['model'])\n",
    "# model = best_model\n",
    "model.eval()  \n",
    "\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():  # Turn off gradients\n",
    "    for sequences, labels in real_loader:\n",
    "        sequences, labels = sequences.to(device).float(), labels.to(device).long()\n",
    "\n",
    "        outputs = model(sequences)\n",
    "\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "for i in range(len(all_predictions)):\n",
    "    if all_predictions[i] == 5 and all_labels[i] == 4:\n",
    "        print(f\"index = {i}\")\n",
    "        print(f\"prediction = {all_predictions[i]}; label = {all_labels[i]}\")\n",
    "\n",
    "# Calculate metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cf = confusion_matrix(all_labels, all_predictions)\n",
    "print(cf)\n",
    "row_sum = np.sum(cf, axis=1, keepdims=True)\n",
    "# print(row_sum)\n",
    "cf = np.divide(cf, row_sum)\n",
    "\n",
    "import seaborn as sns\n",
    "xtick=['L3','L2','L1','M','R1','R2','R3']\n",
    "ytick=['L3','L2','L1','M','R1','R2','R3']\n",
    "labels_heat = [\"{0:.2%}\".format(value) for value in cf.flatten()]\n",
    "labels_heat = np.asarray(labels_heat).reshape(7,7)\n",
    "sns.heatmap(cf,fmt='', annot=labels_heat, cmap='Blues',xticklabels=xtick, yticklabels=ytick)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(classification_report(all_labels, all_predictions))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
